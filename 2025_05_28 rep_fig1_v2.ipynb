{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05811f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import textwrap\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "\n",
    "from datasets import load_dataset  # Hugging Face Datasets\n",
    "from transformers import AlbertTokenizer, AlbertModel, AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7304cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WikiText-103 dataset (version 1)\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27efe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_input(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Get a random input sequence from the dataset that meets minimum length requirements.\n",
    "\n",
    "    dataset: HuggingFace dataset containing text data\n",
    "    tokenizer: Tokenizer to convert text to model input format\n",
    "\n",
    "    Q = why only use sequences longer than 300 tokens?\n",
    "    A = because the model is large and the input sequence needs to be long enough to activate all the parameters\n",
    "\n",
    "    Returns dictionary containing tokenized input with keys:\n",
    "        - 'input_ids': Tensor of token IDs\n",
    "        - 'attention_mask': Tensor of attention masks\n",
    "    \"\"\"\n",
    "    n_train = len(dataset[\"train\"])\n",
    "    while True:\n",
    "        # Randomly select an example from the dataset\n",
    "        it = torch.randint(n_train, (1,)).item()\n",
    "        text = dataset[\"train\"][it][\"text\"]\n",
    "        # Tokenize with padding and truncation\n",
    "        ei = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        # Only use sequences longer than 300 tokens\n",
    "        if ei[\"input_ids\"].shape[1] > 300:\n",
    "            break\n",
    "    return ei\n",
    "\n",
    "\n",
    "def compute_correlations(hidden_states):\n",
    "    \"\"\"\n",
    "    Compute pairwise correlations between token representations for each layer's hidden states.\n",
    "\n",
    "    hidden_states (list):\n",
    "        List of tensors containing hidden states from each layer of the model.\n",
    "        Each tensor has shape (batch_size=1, sequence_length, hidden_dim)\n",
    "\n",
    "    Returns list of tensors containing flattened correlation matrices for each layer.\n",
    "    Each tensor contains the pairwise correlations between all tokens in that layer.\n",
    "    The correlations are computed as cosine similarities between normalized token representations.\n",
    "    \"\"\"\n",
    "    corrs = []\n",
    "    for hs in hidden_states:\n",
    "        # Remove batch dimension and create a copy without gradient tracking\n",
    "        T = hs.squeeze(0).clone().detach().requires_grad_(False)\n",
    "        # Normalize each token's representation to unit length for cosine similarity\n",
    "        T = torch.nn.functional.normalize(T, dim=1)\n",
    "        # Compute pairwise cosine similarities between all tokens\n",
    "        T2 = torch.matmul(T, T.transpose(0, 1))\n",
    "        corrs += [\n",
    "            T2.flatten().cpu(),  # Flatten matrix and move to CPU for plotting\n",
    "        ]\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88ac3c",
   "metadata": {},
   "source": [
    "### Normal Model (24 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62553c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "al_tkz = AlbertTokenizer.from_pretrained(\"albert-xlarge-v2\")\n",
    "al_model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
    "print(al_model.config.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9325b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - \n",
      "Random Input:\n",
      "[CLS] callum blue portrays zod in season nine . zod is first mentioned in season five 's \" arrival \"\n",
      ", when two of his disciples arrive on earth attempting to turn the planet into kryptonian utopia .\n",
      "in the episode \" solitude \" , brainiac attempts to release him from his phantom zone prison , where\n",
      "it is revealed that clark 's biological father jor <unk> -<unk> el placed zod 's spirit after\n",
      "destroying his physical form . in the season five finale , zod is successfully transferred into lex\n",
      "luthor 's body , after clark unknowingly releases him from the phantom zone . clark eventually pulls\n",
      "zod 's spirit out of lex 's body using a kryptonian crystal of his father 's in the season six\n",
      "premiere . in the season eight finale , the kryptonian purple orb , which was used in the season\n",
      "seven finale to destroy the fortress of solitude and remove clark 's powers , appears at the luthor\n",
      "mansion and releases zod in physical form . in the season nine premiere , it is revealed that when\n",
      "zod was released from the orb , he was also accompanied by hundreds of other kryptonians , many of\n",
      "which were scattered across the globe . in addition , none of them were given the powers that\n",
      "typically accompany kryptonians under the yellow sun . season nine episode \" kandor \" reveals that\n",
      "the kandorians are in fact clones created by jor <unk> -<unk> el <unk> at the orders of the\n",
      "kryptonian council <unk> who also corrupted their dna to prevent them from having powers and\n",
      "subsequently enslaving earth . eventually , zod acquires his abilities when clark saves zod 's life\n",
      "by healing a gunshot wound with his own blood in the episode \" conspiracy \" . zod subsequently gives\n",
      "the rest of the kandorians powers , using his blood to renew their lifeforce , and then wages a war\n",
      "on earth in the season nine finale . reluctant at first , clark uses the book of rao to send all\n",
      "kryptonians on earth to another plane of existence where they can live in peace .[SEP]\n",
      "- - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "ei = get_random_input(wikitext, al_tkz)\n",
    "\n",
    "print(\"- \" * 20)\n",
    "print(\"Random Input:\")\n",
    "decoded_text = al_tkz.batch_decode(\n",
    "    ei[\"input_ids\"],\n",
    "    # skip_special_tokens=True\n",
    ")\n",
    "wrapped_text = [textwrap.fill(t, width=100) for t in decoded_text]\n",
    "print(\"\\n\\n\".join(wrapped_text))\n",
    "print(\"- \" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20956954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get output features ... which contains last_hidden_state, pooler_output, hidden_states\n",
    "# hidden_states is a list of tensors, each tensor has shape (batch_size=1, sequence_length, hidden_dim)\n",
    "# last_hidden_state has shape (batch_size=1, sequence_length, hidden_dim)\n",
    "# pooler_output has shape (batch_size=1, hidden_dim)\n",
    "\n",
    "of = al_model(**ei, output_hidden_states=True)\n",
    "correls = compute_correlations(of[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the plots\n",
    "date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder_name = f\"{date_time}_{al_model.config.num_hidden_layers}\"\n",
    "os.makedirs(f\".plots/{folder_name}\", exist_ok=True)\n",
    "\n",
    "# Save the input text as txt file in the folder\n",
    "with open(f\".plots/{folder_name}/input_text.txt\", \"w\") as f:\n",
    "    f.write(decoded_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a791cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save individual histogram plots for each layer's token correlations.\n",
    "# Use adaptive binning based on data distribution and consistent x,y-axis scaling across all plots.\n",
    "\n",
    "# determine global max density (y) value\n",
    "max_density = 0\n",
    "for data in correls:\n",
    "    counts, bin_edges = np.histogram(data, bins=100, density=True)\n",
    "    max_density = max(max_density, max(counts))\n",
    "\n",
    "for i, data in enumerate(correls):\n",
    "    IQR = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    n = len(data)\n",
    "    bin_width = 2 * IQR / n ** (1 / 3)\n",
    "    bins = int((max(data) - min(data)) / bin_width)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(\n",
    "        data,\n",
    "        bins=bins,\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "        color=\"#3658bf\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    plt.title(f\"Layer {i}\", fontsize=12)\n",
    "    plt.xlim(-0.3, 1.05)\n",
    "    plt.ylim(0, max_density)\n",
    "\n",
    "    plt.savefig(f\"./plots/{folder_name}/histogram_layer_{i}.pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b322f",
   "metadata": {},
   "source": [
    "### Larger Model (48 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417840d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "alm2_config = AlbertConfig.from_pretrained(\n",
    "    \"albert-xlarge-v2\", num_hidden_layers=48, num_attention_heads=1\n",
    ")\n",
    "almodel2 = AlbertModel.from_pretrained(\"albert-xlarge-v2\", config=alm2_config)\n",
    "print(almodel2.config.num_hidden_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trafo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
