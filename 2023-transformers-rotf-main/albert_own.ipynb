{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05811f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset # Hugging Face Datasets\n",
    "from transformers import AlbertTokenizer, AlbertModel, AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5f7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WikiText-103 dataset (version 1)\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88ac3c",
   "metadata": {},
   "source": [
    "### Normal Model (24 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62553c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_tkz = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\n",
    "al_model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a797d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(hidden_states):\n",
    "    corrs = []\n",
    "    for hs in hidden_states:\n",
    "        T = hs.squeeze(0).clone().detach().requires_grad_(False)\n",
    "        T = torch.nn.functional.normalize(T, dim=1)\n",
    "        T2 = torch.matmul(T, T.transpose(0, 1))\n",
    "        corrs += [\n",
    "            T2.flatten().cpu(),\n",
    "        ]\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ab7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_input(dataset, tokenizer):\n",
    "    l = len(dataset[\"train\"])\n",
    "    while True:\n",
    "        it = torch.randint(l, (1,)).item()\n",
    "        text = dataset[\"train\"][it][\"text\"]\n",
    "        ei = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        if ei[\"input_ids\"].shape[1] > 300:\n",
    "            break\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78563130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(al_tkz=al_tkz, al_model=al_model):\n",
    "    \"\"\"\n",
    "    Plots histograms for all layers in one 5x5 or 7x6 figure\n",
    "    \"\"\"\n",
    "    ei = get_random_input(wikitext, al_tkz)\n",
    "    print(al_tkz.batch_decode(ei[\"input_ids\"]))\n",
    "    of = al_model(**ei, output_hidden_states=True)\n",
    "    correls = compute_correlations(of[\"hidden_states\"])\n",
    "    if al_model.config.num_hidden_layers < 25:\n",
    "        fig, axes = plt.subplots(5, 5)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(7, 7)\n",
    "    axes = axes.flatten()\n",
    "    for i in range(len(correls)):\n",
    "        axes[i].hist(correls[i], bins=100, density=True, histtype=\"step\")\n",
    "        axes[i].set_title(f\"Layer {i}\")\n",
    "        axes[i].set_xlim(-0.3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a521d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_save(al_tkz=al_tkz, al_model=al_model):\n",
    "    \"\"\"\n",
    "    Creates individual figures for each layer\n",
    "    Saves each histogram as a separate PDF file in a histograms folder\n",
    "    \"\"\"\n",
    "    ei = get_random_input(wikitext, al_tkz)\n",
    "\n",
    "    print('- '*20)\n",
    "    print(\"Random Input:\")\n",
    "    print(al_tkz.batch_decode(ei[\"input_ids\"]))\n",
    "    print('- '*20)\n",
    "    \n",
    "    of = al_model(**ei, output_hidden_states=True)\n",
    "    correls = compute_correlations(of[\"hidden_states\"])\n",
    "\n",
    "    # Create a directory to save the plots\n",
    "    os.makedirs(\"histograms\", exist_ok=True)\n",
    "\n",
    "    # Determine the global maximum density value\n",
    "    max_density = 0\n",
    "    for data in correls:\n",
    "        counts, bin_edges = np.histogram(data, bins=100, density=True)\n",
    "        max_density = max(max_density, max(counts))\n",
    "\n",
    "    for i, data in enumerate(correls):\n",
    "        IQR = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "        n = len(data)\n",
    "        bin_width = 2 * IQR / n ** (1 / 3)\n",
    "        bins = int((max(data) - min(data)) / bin_width)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(\n",
    "            data,\n",
    "            bins=bins,\n",
    "            density=True,\n",
    "            histtype=\"step\",\n",
    "            color=\"#3658bf\",\n",
    "            linewidth=1.5,\n",
    "        )\n",
    "        plt.title(f\"Layer {i}\", fontsize=16)\n",
    "        plt.xlim(-0.3, 1.05)\n",
    "        plt.ylim(0, max_density)  # Set a consistent y-axis limit\n",
    "\n",
    "        plt.savefig(f\"./histograms/histogram_layer_{i}.pdf\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17347be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - \n",
      "Random Input:\n",
      "['[CLS] in march 1940 , hancock \\'s directorate of works and buildings was transferred from the office of the chief of the air staff to the newly formed organisation and equipment branch under air marshal richard williams . considered a key part of the air force \\'s expansion during the early part of world war ii , \" works and bricks \" quickly absorbed all staff with civil engineering and building experience in the raaf active reserve . as director , hancock was responsible for surveying and developing a military aerodrome at evans head , near the queensland and new south wales border , which became home to no. 1 bombing and gunnery school ( no. 1<unk> ) . promoted to wing commander , he held command of no. 1<unk> , operating fairey battle single <unk> -<unk> engined bombers , from august 1940 until november 1941 . he was promoted to acting group captain in april 1941 . appointed an officer of the order of the british empire ( obe ) on 1 january 1942 , hancock became assistant director of plans at allied air forces headquarters , south west pacific area , that april . he was made director of plans at the air force \\'s main operational formation , raaf command , when it was established in september . in 1943 â€“ 44 , he served as staff officer administration for western area command , which maintained two bomber squadrons for anti <unk> -<unk> submarine patrols and two fighter squadrons to guard against possible attack on the mainland by japanese carrier <unk> -<unk> borne aircraft .[SEP]']\n",
      "- - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "plot_histograms_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687edbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# al_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de372f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check norm of output tokens\n",
    "# The norm is not exactly the same because the LayerNorm\n",
    "# that is applied at the end also has trainable diagonal matrix \\gamma and vector \\beta which are used as follows\n",
    "# (on each token)\n",
    "# \\tilde x = (x - mean(x))/sqrt(var(x)) * \\gamma + \\beta (here token is a row vector)\n",
    "\n",
    "# ei = get_random_input(wikitext, al_tkz)\n",
    "# print(al_tkz.batch_decode(ei[\"input_ids\"]))\n",
    "# of = al_model(**ei, output_hidden_states=True)\n",
    "# of[\"hidden_states\"][24].var(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b322f",
   "metadata": {},
   "source": [
    "### Larger Model (48 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417840d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "alm2_config = AlbertConfig.from_pretrained(\n",
    "    \"albert-xlarge-v2\", num_hidden_layers=48, num_attention_heads=1\n",
    ")\n",
    "almodel2 = AlbertModel.from_pretrained(\"albert-xlarge-v2\", config=alm2_config)\n",
    "print(almodel2.config.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b4afb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - \n",
      "Random Input:\n",
      "['[CLS] \" i \\'m her slave \" is a heroin anecdote with lyrics narrated by a subjugate lover . music critic greg kot cites \" turn on the water \" as an example of when \" the twisted narrator is the victim \" and \" cast adrift \" in dulli \\'s lyrics . inspired by a paranoid breakup , \" conjure me \" is told from the perspective of an aggressive predator and obscure object of desire . on \" kiss the floor \" , the narrator recounts stealing a girl \\'s virginity and avoiding her brothers . \" this is my confession \" has a theme of absolution . the lyrics depicts it as an empty sexual experience : \" shove my head against the door , crawl inside and kiss the floor / waiting for the sun again , drink it , smoke it , stick it in . \" \" the temple \" is a cover of the song of the same name from the 1970 rock opera jesus christ superstar . dulli became a fan of the rock opera as a child when his babysitter played it . \" let me lie to you \" has lyrics expressing passive cruelty and subtle manipulation . \" tonight \" depicts a peaceful night as the backdrop for the narrator \\'s corrupt one <unk> -<unk> track mind : \" follow me down to the bushes , dear / no one will know , we \\'ll disappear / i \\'ll hold your hand , we \\'ll never tell / our private little trip to hell \" . the album \\'s hidden track \" miles iz ded \" is about seduction and alcohol , with a last call sensibility and despairing tone .[SEP]']\n",
      "- - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "plot_histograms_save(al_model=almodel2)\n",
    "# plot_histograms(al_model = almodel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b385d7",
   "metadata": {},
   "source": [
    "### Decomposing Internal Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f896ed",
   "metadata": {},
   "source": [
    "This section appears to be a debugging/exploration section where the author is:\n",
    "\n",
    "- Understanding the internal structure of the ALBERT model\n",
    "- Testing how the attention mechanism processes inputs\n",
    "- Examining the shapes and transformations of tensors through the model\n",
    "- Looking at the actual weight matrices used in the attention mechanism\n",
    "\n",
    "This kind of exploration is common when trying to understand or debug transformer models, as it helps to verify that the internal components are working as expected and to understand how the model processes information at a detailed level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a89a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), 16, 2048, 2048)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decomposing AlbertModel\n",
    "al_transfo = al_model.encoder\n",
    "al_layer = al_transfo.albert_layer_groups[0].albert_layers[0]\n",
    "al_attention = al_layer.attention\n",
    "(\n",
    "    al_attention.pruned_heads,\n",
    "    al_attention.num_attention_heads,\n",
    "    al_attention.all_head_size,\n",
    "    al_attention.hidden_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b3edac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] the subject matter of the carvings of the central brackets as misericords is very varied , but with many common themes recurring in different churches . typically , the themes are less unified and less directly related to the bible and christian theology than are the themes of small sculptures seen elsewhere within churches , such as those on bosses . this is much the case at wells , where none of the misericord carvings is directly based on a bible story . the subjects , chosen either by the woodcarver , or perhaps by the individual paying for the stall , have no over <unk>-<unk> riding theme . the sole unifying element is the roundels on each side of the pictorial subject , which are all elaborately carved foliage , in most cases formal and stylised in the later decorated manner , but with several examples of naturalistic foliage , including roses and<unk> . many of the subjects carry traditional interpretations . the image of the \" pelican in her piety \" ( believed to feed her young on her own blood ) is a recognised symbol for christ \\'s love for the church . a cat playing with a mouse may represent the devil snaring a human soul . other subjects illustrate popular fables or sayings such as \" when the fox preaches , look to your geese \" . many of the subjects are depictions of animals , some of which may symbolise a human vice or virtue , or an aspect of faith .[SEP]']\n"
     ]
    }
   ],
   "source": [
    "ei = get_random_input(wikitext, al_tkz)\n",
    "print(al_tkz.batch_decode(ei[\"input_ids\"]))\n",
    "of = al_model(**ei, output_hidden_states=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f715c9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 321, 2048])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "of[\"hidden_states\"][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "253842f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2048])\n"
     ]
    }
   ],
   "source": [
    "test = torch.arange(2048).reshape(1, 1, -1)\n",
    "print(test.shape)\n",
    "al_attention.transpose_for_scores(test).shape;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74b48ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
       "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
       "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
       "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
       "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
       "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
       "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
       "        254, 255])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_attention.transpose_for_scores(test)[0, 1, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf827332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0182,  0.0070, -0.0499,  ...,  0.0107, -0.0472,  0.0081],\n",
       "        [-0.0077, -0.0219,  0.0191,  ...,  0.0136, -0.0058,  0.0144],\n",
       "        [-0.0141, -0.0141,  0.0194,  ...,  0.0275,  0.0075, -0.0503],\n",
       "        ...,\n",
       "        [ 0.0342,  0.0207,  0.0073,  ...,  0.0331, -0.0137,  0.0200],\n",
       "        [ 0.0521,  0.0029,  0.0261,  ...,  0.0122,  0.0592, -0.0423],\n",
       "        [ 0.0113,  0.0855, -0.0034,  ..., -0.0119,  0.0079, -0.0166]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_attention.value.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trafo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
